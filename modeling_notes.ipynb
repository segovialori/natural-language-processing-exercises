{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Modeling\n",
    "\n",
    "The \"big idea\" of modeling is to determine what a document is all about; which words are important or not. The main task is to determine the weight of each word, relative to the document.\n",
    "\n",
    "\n",
    "## What\n",
    "- Introducing our _Dramatis Personae_, the characters in our play:\n",
    "    - `Term frequency` is a direct way to measure what a document is about, but it over-emphasizes common terms. Consider term frequency the baseline, kinda like how median, median, or mode can be baselines. It's at least somewhere to start, even if it's a blunt tool w/ some issues.\n",
    "    - `TF` = # times a word occurs divided by the total amount of words. \n",
    "    - `Bag of words` is a representation of a document as a vector, where the values indicate word frequency.\n",
    "        ```\n",
    "        string = \"Mary had a little lamb, little lamb, little lamb.\"\n",
    "        string = string.replace(\",\", \"\")\n",
    "        words = string.split()\n",
    "        bag_of_words = pd.Series(words).value_counts()\n",
    "        ```\n",
    "    - Word clouds are a visual bag of words with larger font sizes representing higher term frequency\n",
    "    - Inverse Document Frequency, `IDF`, tells us how much information a word provides. \n",
    "        - A higher IDF means that a word provides more information. That is, it is more relevant within a single document.\n",
    "        - As the number of documents that a word appears in increases, the IDF value decreases.\n",
    "        - Example: if \"Codeup\" appears frequently in every document in a list of documents, then the word doesn't add much new information on any given individual document.\n",
    "        - Example: if \"scholarship\" shows up a whole bunch one one or two documents, but not frequently across the corups of documents, then we can conclude that that word conveys more meaning.\n",
    "        \n",
    "    - `TF-IDF` is the product of `tf * idf` and is \n",
    "\n",
    "\n",
    "## So What?\n",
    "- Determining what a document is about is both valuable and challening.\n",
    "- Term frequency is super sensitive to noise\n",
    "- TF-IDF is super common and has been used in the majority of text based recommendation systems. See [tf-idf in Wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n",
    "\n",
    "## Now What?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tf-idf is the product of tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_name</th>\n",
       "      <th>line</th>\n",
       "      <th>character</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Encounter at Farpoint</td>\n",
       "      <td>Difficult? Simply solve the mystery of Farpoi...</td>\n",
       "      <td>DATA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Encounter at Farpoint</td>\n",
       "      <td>As simple as that.</td>\n",
       "      <td>PICARD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Encounter at Farpoint</td>\n",
       "      <td>Farpoint Station. Even the name sounds myster...</td>\n",
       "      <td>TROI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Encounter at Farpoint</td>\n",
       "      <td>It's hardly simple, Data, to negotiate a frie...</td>\n",
       "      <td>PICARD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Encounter at Farpoint</td>\n",
       "      <td>Inquiry. The word snoop?</td>\n",
       "      <td>DATA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51983</th>\n",
       "      <td>All Good Things</td>\n",
       "      <td>Of course. Have a seat.</td>\n",
       "      <td>RIKER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51984</th>\n",
       "      <td>All Good Things</td>\n",
       "      <td>Would you care to deal, sir?</td>\n",
       "      <td>DATA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51985</th>\n",
       "      <td>All Good Things</td>\n",
       "      <td>Oh, er, thank you, Mister Data. Actually, I u...</td>\n",
       "      <td>PICARD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51986</th>\n",
       "      <td>All Good Things</td>\n",
       "      <td>You were always welcome.</td>\n",
       "      <td>TROI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51987</th>\n",
       "      <td>All Good Things</td>\n",
       "      <td>So. Five card stud, nothing wild, and the sky...</td>\n",
       "      <td>PICARD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38931 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                episode_name  \\\n",
       "0      Encounter at Farpoint   \n",
       "1      Encounter at Farpoint   \n",
       "2      Encounter at Farpoint   \n",
       "3      Encounter at Farpoint   \n",
       "4      Encounter at Farpoint   \n",
       "...                      ...   \n",
       "51983        All Good Things   \n",
       "51984        All Good Things   \n",
       "51985        All Good Things   \n",
       "51986        All Good Things   \n",
       "51987        All Good Things   \n",
       "\n",
       "                                                    line character  \n",
       "0       Difficult? Simply solve the mystery of Farpoi...      DATA  \n",
       "1                                     As simple as that.    PICARD  \n",
       "2       Farpoint Station. Even the name sounds myster...      TROI  \n",
       "3       It's hardly simple, Data, to negotiate a frie...    PICARD  \n",
       "4                               Inquiry. The word snoop?      DATA  \n",
       "...                                                  ...       ...  \n",
       "51983                            Of course. Have a seat.     RIKER  \n",
       "51984                       Would you care to deal, sir?      DATA  \n",
       "51985   Oh, er, thank you, Mister Data. Actually, I u...    PICARD  \n",
       "51986                           You were always welcome.      TROI  \n",
       "51987   So. Five card stud, nothing wild, and the sky...    PICARD  \n",
       "\n",
       "[38931 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#grab the first 15 characters with most line\n",
    "df = pd.read_csv(\"tng.txt\")\n",
    "\n",
    "top_15_characters = df.character.value_counts().index[0:15]\n",
    "\n",
    "top_15 = df[df.character.isin(top_15_characters)]\n",
    "top_15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-5719cc07d569>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-5719cc07d569>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    return \" \".join[wnl.lemmatize(word) for word in words if word not in stopwords]\u001b[0m\n\u001b[0m                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "ADDITIONAL_STOPWORDS = ['r', 'u', '2', 'ltgt'] #ltgt is html artifact\n",
    "\n",
    "def clean(text):\n",
    "    'A simple function to cleanup text data'\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    stopwords = nltk.corpus.stopwords.words('english') + ADDITIONAL_STOPWORDS\n",
    "    text = (unicodedata.normalize('NFKD', text)\n",
    "             .encode('ascii', 'ignore')\n",
    "             .decode('utf-8', 'ignore')\n",
    "             .lower())\n",
    "    words = re.sub(r'[^\\w\\s]', '', text).split()\n",
    "    return \" \".join[wnl.lemmatize(word) for word in words if word not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use this split function later to create in-sample and out-of-sample datasets for modeling\n",
    "def split(df, stratify_by=None):\n",
    "    \"\"\"\n",
    "    3 way split for train, validate, and test datasets\n",
    "    To stratify, send in a column name\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    train, test = train_test_split(df, test_size=.2, random_state=123, stratify=df[stratify_by])\n",
    "    \n",
    "    train, validate = train_test_split(train, test_size=.3, random_state=123, stratify=train[stratify_by])\n",
    "    \n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- end goal: predicting what character said what line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test = split(top_15, 'character')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup our X variables\n",
    "X_train = train.line\n",
    "X_validate = validate.line\n",
    "X_test = test.line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup our y variables\n",
    "y_train = train.character\n",
    "y_validate = validate.character\n",
    "y_test = test.character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All text\n",
    "#\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#like one hot encodoing\n",
    "#produces a matric for each line\n",
    "#not a scaler, but basically like an encoder\n",
    "\n",
    "# Create the tfidf vectorizer object\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# Fit on the training data\n",
    "tfidf.fit(X_train)\n",
    "\n",
    "#use the object\n",
    "X_train_vectorized = tfidf.transform(X_train)\n",
    "X_validate_vectorized = tfidf.transform(X_validate)\n",
    "X_test_vectorized = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LogisticRegression().fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame(dict(actual=y_train))\n",
    "validate = pd.DataFrame(dict(actual=y_validate))\n",
    "test = pd.DataFrame(dict(actual=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['predicted'] = lm.predict(X_train_vectorized)\n",
    "validate[\"predicted\"] = lm.predict(X_validate_vectorized)\n",
    "test['predicted'] = lm.predict(X_test_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Accuracy\n",
    "(train.actual == train.predicted).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(validate.actual == validate.predicted).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = pd.Series([\n",
    "    \"we have a responsibility\", \n",
    "    \"set phasers to stun\", \n",
    "    \"the warp drive is about to go critical\", \n",
    "    \"What does it mean to be human? I cannot calculate feelings\", \n",
    "    \"Romulan bird of prey decloaking off the port bow\"\n",
    "])\n",
    "X = tfidf.transform(lines)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
