{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The end result of this exercise should be a file named prepare.py that defines the requested functions.\n",
    "\n",
    "In this exercise we will be defining some functions to prepare textual data. These functions should apply equally well to both the codeup blog articles and the news articles that were previously acquired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import acquire as a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acquire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lorisegovia/codeup-data-science/natural-language-processing-exercises/acquire.py:92: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 92 of the file /Users/lorisegovia/codeup-data-science/natural-language-processing-exercises/acquire.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  soup = BeautifulSoup(response.text)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>India underestimated the coronavirus: Raghuram...</td>\n",
       "      <td>Speaking about India's second COVID-19 wave, f...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Air India pilots demand vaccination on priorit...</td>\n",
       "      <td>Indian Commercial Pilots Association (ICPA) on...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>World's biggest jeweller says it will no longe...</td>\n",
       "      <td>Pandora, the world's biggest jeweller, has sai...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>South Korea's richest woman gets fortune worth...</td>\n",
       "      <td>South Korea’s richest woman Hong Ra-hee added ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Will supply 11 cr doses to states, pvt hospita...</td>\n",
       "      <td>Serum Institute of India (SII) CEO Adar Poonaw...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  India underestimated the coronavirus: Raghuram...   \n",
       "1  Air India pilots demand vaccination on priorit...   \n",
       "2  World's biggest jeweller says it will no longe...   \n",
       "3  South Korea's richest woman gets fortune worth...   \n",
       "4  Will supply 11 cr doses to states, pvt hospita...   \n",
       "\n",
       "                                             content  category  \n",
       "0  Speaking about India's second COVID-19 wave, f...  business  \n",
       "1  Indian Commercial Pilots Association (ICPA) on...  business  \n",
       "2  Pandora, the world's biggest jeweller, has sai...  business  \n",
       "3  South Korea’s richest woman Hong Ra-hee added ...  business  \n",
       "4  Serum Institute of India (SII) CEO Adar Poonaw...  business  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df = a.get_all_news_articles(a.categories)\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Define a function named basic_clean. It should take in a string and apply some basic text cleaning to it:\n",
    "- Lowercase everything\n",
    "- Normalize unicode characters\n",
    "- Replace anything that is not a letter, number, whitespace or a single quote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to clean\n",
    "def basic_clean(string):\n",
    "    \"\"\"\n",
    "    This function takes in one argument (string) and will apply\n",
    "    some basic text cleaning to it:\n",
    "    1. lowercase everything\n",
    "    2. normalize unicode characters\n",
    "    3. replace anything that is not a letter, number, whitespace,\n",
    "    or a single quote\n",
    "    \"\"\"\n",
    "    lowercase = string.lower()\n",
    "    normalize = unicodedata.normalize('NFKD', lowercase)\\\n",
    "    .encode('ascii', 'ignore')\\\n",
    "    .decode('utf-8', 'ignore')\n",
    "    remove_special = re.sub(r\"[^a-z0-9'\\s]\", '', normalize)\n",
    "    clean_string = remove_special\n",
    "    return clean_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"indian commercial pilots association icpa on tuesday said if air india fails to set up vaccination camps on a panindia basis forflying crew above the age of 18 years on priority we'll stop work in a letter to the airline's management icpa added with no healthcare supportno insurancewe're in no position to continue risking lives of our pilots without vaccination\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make sure function works\n",
    "string = news_df.content[1]\n",
    "clean = basic_clean(string)\n",
    "clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pandora, the world\\'s biggest jeweller, has said that it\\'ll stop using mined diamonds and focus on laboratory-made ones, which are affordable and sustainable. \"Diamonds are not only forever, but for everyone,\" Pandora CEO Alexander Lacik said. This comes amid growing demand for alternatives to mined diamonds amid concerns over unethical practices in mining industry, including human rights abuses.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#grab some text to practice with\n",
    "string = news_df.content[2]\n",
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pandora, the world\\'s biggest jeweller, has said that it\\'ll stop using mined diamonds and focus on laboratory-made ones, which are affordable and sustainable. \"diamonds are not only forever, but for everyone,\" pandora ceo alexander lacik said. this comes amid growing demand for alternatives to mined diamonds amid concerns over unethical practices in mining industry, including human rights abuses.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lowercase the text\n",
    "lowercase = string.lower()\n",
    "lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize: Removing accented characters or non-ASCII characters\n",
    "normalize = unicodedata.normalize('NFKD', lowercase)\\\n",
    ".encode('ascii', 'ignore')\\\n",
    ".decode('utf-8', 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pandora, the world\\'s biggest jeweller, has said that it\\'ll stop using mined diamonds and focus on laboratory-made ones, which are affordable and sustainable. \"diamonds are not only forever, but for everyone,\" pandora ceo alexander lacik said. this comes amid growing demand for alternatives to mined diamonds amid concerns over unethical practices in mining industry, including human rights abuses.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"pandora the world's biggest jeweller has said that it'll stop using mined diamonds and focus on laboratorymade ones which are affordable and sustainable diamonds are not only forever but for everyone pandora ceo alexander lacik said this comes amid growing demand for alternatives to mined diamonds amid concerns over unethical practices in mining industry including human rights abuses\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove special characters\n",
    "remove_special = re.sub(r\"[^a-z0-9'\\s]\", '', normalize)\n",
    "remove_special"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define a function named tokenize. It should take in a string and tokenize all the words in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to tokenize\n",
    "def tokenize(string):\n",
    "    \"\"\"\n",
    "    This function will take in one argument(string) and will\n",
    "    tokenize all words in the string.\n",
    "    \"\"\"\n",
    "    #create tokenizer\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "    #use tokenizer\n",
    "    tokens = tokenizer.tokenize(basic_clean(string), return_str = True)\n",
    "    \n",
    "    return tokens\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"indian commercial pilots association icpa on tuesday said if air india fails to set up vaccination camps on a panindia basis forflying crew above the age of 18 years on priority we ' ll stop work in a letter to the airline ' s management icpa added with no healthcare supportno insurancewe ' re in no position to continue risking lives of our pilots without vaccination\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make sure function works\n",
    "string = news_df.content[0]\n",
    "tokenize(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create tokenizer\n",
    "tokenizer = nltk.tokenize.ToktokTokenizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['indian',\n",
       " 'commercial',\n",
       " 'pilots',\n",
       " 'association',\n",
       " 'icpa',\n",
       " 'on',\n",
       " 'tuesday',\n",
       " 'said',\n",
       " 'if',\n",
       " 'air',\n",
       " 'india',\n",
       " 'fails',\n",
       " 'to',\n",
       " 'set',\n",
       " 'up',\n",
       " 'vaccination',\n",
       " 'camps',\n",
       " 'on',\n",
       " 'a',\n",
       " 'panindia',\n",
       " 'basis',\n",
       " 'forflying',\n",
       " 'crew',\n",
       " 'above',\n",
       " 'the',\n",
       " 'age',\n",
       " 'of',\n",
       " '18',\n",
       " 'years',\n",
       " 'on',\n",
       " 'priority',\n",
       " 'we',\n",
       " \"'\",\n",
       " 'll',\n",
       " 'stop',\n",
       " 'work',\n",
       " 'in',\n",
       " 'a',\n",
       " 'letter',\n",
       " 'to',\n",
       " 'the',\n",
       " 'airline',\n",
       " \"'\",\n",
       " 's',\n",
       " 'management',\n",
       " 'icpa',\n",
       " 'added',\n",
       " 'with',\n",
       " 'no',\n",
       " 'healthcare',\n",
       " 'supportno',\n",
       " 'insurancewe',\n",
       " \"'\",\n",
       " 're',\n",
       " 'in',\n",
       " 'no',\n",
       " 'position',\n",
       " 'to',\n",
       " 'continue',\n",
       " 'risking',\n",
       " 'lives',\n",
       " 'of',\n",
       " 'our',\n",
       " 'pilots',\n",
       " 'without',\n",
       " 'vaccination']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the tokenizer\n",
    "tokenizer.tokenize(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokens = tokenizer.tokenize(basic_clean(text))\n",
    "#tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Speaking about India\\'s second COVID-19 wave, former RBI Governor Raghuram Rajan said, \"I think what went wrong was simply [that]...we underestimated the virus and its ability to adapt.\" After the first wave, \"there was a sense that we had endured the worst...and we had come through, and it was time to open up, and that complacency hurt us\", he added.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define a function named stem. It should accept some text and return the text after applying stemming to all the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to stem\n",
    "def stem(string):\n",
    "    # Create porter stemmer.\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    # Apply the stemmer to each word in our string.\n",
    "    stems = [ps.stem(word) for word in string.split()]\n",
    "    \n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['indian',\n",
       " 'commerci',\n",
       " 'pilot',\n",
       " 'associ',\n",
       " 'icpa',\n",
       " 'on',\n",
       " 'tuesday',\n",
       " 'said',\n",
       " 'if',\n",
       " 'air',\n",
       " 'india',\n",
       " 'fail',\n",
       " 'to',\n",
       " 'set',\n",
       " 'up',\n",
       " 'vaccin',\n",
       " 'camp',\n",
       " 'on',\n",
       " 'a',\n",
       " 'panindia',\n",
       " 'basi',\n",
       " 'forfli',\n",
       " 'crew',\n",
       " 'abov',\n",
       " 'the',\n",
       " 'age',\n",
       " 'of',\n",
       " '18',\n",
       " 'year',\n",
       " 'on',\n",
       " 'prioriti',\n",
       " \"we'll\",\n",
       " 'stop',\n",
       " 'work',\n",
       " 'in',\n",
       " 'a',\n",
       " 'letter',\n",
       " 'to',\n",
       " 'the',\n",
       " \"airline'\",\n",
       " 'manag',\n",
       " 'icpa',\n",
       " 'ad',\n",
       " 'with',\n",
       " 'no',\n",
       " 'healthcar',\n",
       " 'supportno',\n",
       " \"insurancewe'r\",\n",
       " 'in',\n",
       " 'no',\n",
       " 'posit',\n",
       " 'to',\n",
       " 'continu',\n",
       " 'risk',\n",
       " 'live',\n",
       " 'of',\n",
       " 'our',\n",
       " 'pilot',\n",
       " 'without',\n",
       " 'vaccin']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test function\n",
    "stem(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create porter stemmer.\n",
    "ps = nltk.porter.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"indian commercial pilots association icpa on tuesday said if air india fails to set up vaccination camps on a panindia basis forflying crew above the age of 18 years on priority we'll stop work in a letter to the airline's management icpa added with no healthcare supportno insurancewe're in no position to continue risking lives of our pilots without vaccin\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the stemmer to each word in our string.\n",
    "#stem it word by word\n",
    "stems = ps.stem(clean)\n",
    "stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['indian',\n",
       " 'commerci',\n",
       " 'pilot',\n",
       " 'associ',\n",
       " 'icpa',\n",
       " 'on',\n",
       " 'tuesday',\n",
       " 'said',\n",
       " 'if',\n",
       " 'air']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the stemmer to each word in our string.\n",
    "stems = [ps.stem(word) for word in clean.split()]\n",
    "stems[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"indian commerci pilot associ icpa on tuesday said if air india fail to set up vaccin camp on a panindia basi forfli crew abov the age of 18 year on prioriti we'll stop work in a letter to the airline' manag icpa ad with no healthcar supportno insurancewe'r in no posit to continu risk live of our pilot without vaccin\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join our lists of words into a string again\n",
    "article_stemmed = ' '.join(stems)\n",
    "article_stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Define a function named lemmatize. It should accept some text and return the text after applying lemmatization to each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to lemmatize\n",
    "def lemmatize(string):\n",
    "    # Download the first time.\n",
    "    nltk.download('wordnet')\n",
    "    # Create the Lemmatizer.\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    # Use the lemmatizer on each word in the list of words we created by using split.\n",
    "    lemmas = [wnl.lemmatize(word) for word in string.split()]\n",
    "    \n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/lorisegovia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['indian',\n",
       " 'commercial',\n",
       " 'pilot',\n",
       " 'association',\n",
       " 'icpa',\n",
       " 'on',\n",
       " 'tuesday',\n",
       " 'said',\n",
       " 'if',\n",
       " 'air',\n",
       " 'india',\n",
       " 'fails',\n",
       " 'to',\n",
       " 'set',\n",
       " 'up',\n",
       " 'vaccination',\n",
       " 'camp',\n",
       " 'on',\n",
       " 'a',\n",
       " 'panindia',\n",
       " 'basis',\n",
       " 'forflying',\n",
       " 'crew',\n",
       " 'above',\n",
       " 'the',\n",
       " 'age',\n",
       " 'of',\n",
       " '18',\n",
       " 'year',\n",
       " 'on',\n",
       " 'priority',\n",
       " \"we'll\",\n",
       " 'stop',\n",
       " 'work',\n",
       " 'in',\n",
       " 'a',\n",
       " 'letter',\n",
       " 'to',\n",
       " 'the',\n",
       " \"airline's\",\n",
       " 'management',\n",
       " 'icpa',\n",
       " 'added',\n",
       " 'with',\n",
       " 'no',\n",
       " 'healthcare',\n",
       " 'supportno',\n",
       " \"insurancewe're\",\n",
       " 'in',\n",
       " 'no',\n",
       " 'position',\n",
       " 'to',\n",
       " 'continue',\n",
       " 'risking',\n",
       " 'life',\n",
       " 'of',\n",
       " 'our',\n",
       " 'pilot',\n",
       " 'without',\n",
       " 'vaccination']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check funciton\n",
    "lemmatize(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/lorisegovia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the first time.\n",
    "nltk.download('wordnet') #words that exist in the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Lemmatizer.\n",
    "\n",
    "wnl = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the lemmatizer on each word in the list of words we created by using split.\n",
    "\n",
    "lemmas = [wnl.lemmatize(word) for word in string.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Define a function named remove_stopwords. It should accept some text and return the text after removing all the stopwords.\n",
    "- This function should define two optional parameters, extra_words and exclude_words. These parameters should define any additional stop words to include, and any words that we don't want to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to remove stop words\n",
    "def remove_stopwords(string, extra_words=[], exclude_words=[]):\n",
    "    \"\"\"\n",
    "    This function will take in three arguments string, extra_words,\n",
    "    and exclude words.  \n",
    "    \"\"\"\n",
    "    # Create stopword_list.\n",
    "    stopword_list = stopwords.words('english')\n",
    "    \n",
    "    # Remove 'exclude_words' from stopword_list to keep these in my text.\n",
    "    stopword_list = set(stopword_list) - set(exclude_words)\n",
    "    \n",
    "    # Add in 'extra_words' to stopword_list.\n",
    "    stopword_list = stopword_list.union(set(extra_words))\n",
    "    \n",
    "    # Split words in string.\n",
    "    words = string.split()\n",
    "    \n",
    "    # Create a list of words from my string with stopwords removed and assign to variable.\n",
    "    filtered_words = [word for word in words if word not in stopword_list]\n",
    "    \n",
    "    # Join words in the list back into strings and assign to a variable.\n",
    "    string_without_stopwords = ' '.join(filtered_words)\n",
    "    \n",
    "    return string_without_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Use your data from the acquire to produce a dataframe of the news articles. Name the dataframe news_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lorisegovia/codeup-data-science/natural-language-processing-exercises/acquire.py:92: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 92 of the file /Users/lorisegovia/codeup-data-science/natural-language-processing-exercises/acquire.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  soup = BeautifulSoup(response.text)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Air India pilots demand vaccination on priorit...</td>\n",
       "      <td>Indian Commercial Pilots Association (ICPA) on...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>India underestimated the coronavirus: Raghuram...</td>\n",
       "      <td>Speaking about India's second COVID-19 wave, f...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>World's biggest jeweller says it will no longe...</td>\n",
       "      <td>Pandora, the world's biggest jeweller, has sai...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>South Korea's richest woman gets fortune worth...</td>\n",
       "      <td>South Korea’s richest woman Hong Ra-hee added ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Will supply 11 cr doses to states, pvt hospita...</td>\n",
       "      <td>Serum Institute of India (SII) CEO Adar Poonaw...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Air India pilots demand vaccination on priorit...   \n",
       "1  India underestimated the coronavirus: Raghuram...   \n",
       "2  World's biggest jeweller says it will no longe...   \n",
       "3  South Korea's richest woman gets fortune worth...   \n",
       "4  Will supply 11 cr doses to states, pvt hospita...   \n",
       "\n",
       "                                             content  category  \n",
       "0  Indian Commercial Pilots Association (ICPA) on...  business  \n",
       "1  Speaking about India's second COVID-19 wave, f...  business  \n",
       "2  Pandora, the world's biggest jeweller, has sai...  business  \n",
       "3  South Korea’s richest woman Hong Ra-hee added ...  business  \n",
       "4  Serum Institute of India (SII) CEO Adar Poonaw...  business  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df = a.get_all_news_articles(a.categories)\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Make another dataframe for the Codeup blog posts. Name the dataframe codeup_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lorisegovia/codeup-data-science/natural-language-processing-exercises/acquire.py:16: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 16 of the file /Users/lorisegovia/codeup-data-science/natural-language-processing-exercises/acquire.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  soup = BeautifulSoup(response.text)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>published_date</th>\n",
       "      <th>blog_image</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Codeup’s Data Science Career Accelerator is Here!</td>\n",
       "      <td>September 30, 2018</td>\n",
       "      <td>https://codeup.com/wp-content/uploads/2018/10/...</td>\n",
       "      <td>The rumors are true! The time has arrived. Cod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Science Myths</td>\n",
       "      <td>October 31, 2018</td>\n",
       "      <td>https://codeup.com/wp-content/uploads/2018/10/...</td>\n",
       "      <td>By Dimitri Antoniou and Maggie Giust\\nData Sci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Science VS Data Analytics: What’s The Dif...</td>\n",
       "      <td>October 17, 2018</td>\n",
       "      <td>https://codeup.com/wp-content/uploads/2018/10/...</td>\n",
       "      <td>By Dimitri Antoniou\\nA week ago, Codeup launch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10 Tips to Crush It at the SA Tech Job Fair</td>\n",
       "      <td>August 14, 2018</td>\n",
       "      <td>None</td>\n",
       "      <td>SA Tech Job Fair\\nThe third bi-annual San Anto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Competitor Bootcamps Are Closing. Is the Model...</td>\n",
       "      <td>August 14, 2018</td>\n",
       "      <td>None</td>\n",
       "      <td>Competitor Bootcamps Are Closing. Is the Model...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title      published_date  \\\n",
       "0  Codeup’s Data Science Career Accelerator is Here!  September 30, 2018   \n",
       "1                                 Data Science Myths    October 31, 2018   \n",
       "2  Data Science VS Data Analytics: What’s The Dif...    October 17, 2018   \n",
       "3        10 Tips to Crush It at the SA Tech Job Fair     August 14, 2018   \n",
       "4  Competitor Bootcamps Are Closing. Is the Model...     August 14, 2018   \n",
       "\n",
       "                                          blog_image  \\\n",
       "0  https://codeup.com/wp-content/uploads/2018/10/...   \n",
       "1  https://codeup.com/wp-content/uploads/2018/10/...   \n",
       "2  https://codeup.com/wp-content/uploads/2018/10/...   \n",
       "3                                               None   \n",
       "4                                               None   \n",
       "\n",
       "                                             content  \n",
       "0  The rumors are true! The time has arrived. Cod...  \n",
       "1  By Dimitri Antoniou and Maggie Giust\\nData Sci...  \n",
       "2  By Dimitri Antoniou\\nA week ago, Codeup launch...  \n",
       "3  SA Tech Job Fair\\nThe third bi-annual San Anto...  \n",
       "4  Competitor Bootcamps Are Closing. Is the Model...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codeup_df = a.acquire_codeup_blog()\n",
    "codeup_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. For each dataframe, produce the following columns:\n",
    "- title to hold the title\n",
    "- original to hold the original article/post content\n",
    "- clean to hold the normalized and tokenized original with the stopwords removed.\n",
    "- stemmed to hold the stemmed version of the cleaned data.\n",
    "- lemmatized to hold the lemmatized version of the cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "codeup_df['clean'] = codeup_df.content.apply(basic_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>published_date</th>\n",
       "      <th>blog_image</th>\n",
       "      <th>content</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Codeup’s Data Science Career Accelerator is Here!</td>\n",
       "      <td>September 30, 2018</td>\n",
       "      <td>https://codeup.com/wp-content/uploads/2018/10/...</td>\n",
       "      <td>The rumors are true! The time has arrived. Cod...</td>\n",
       "      <td>the rumors are true the time has arrived codeu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Science Myths</td>\n",
       "      <td>October 31, 2018</td>\n",
       "      <td>https://codeup.com/wp-content/uploads/2018/10/...</td>\n",
       "      <td>By Dimitri Antoniou and Maggie Giust\\nData Sci...</td>\n",
       "      <td>by dimitri antoniou and maggie giust\\ndata sci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Science VS Data Analytics: What’s The Dif...</td>\n",
       "      <td>October 17, 2018</td>\n",
       "      <td>https://codeup.com/wp-content/uploads/2018/10/...</td>\n",
       "      <td>By Dimitri Antoniou\\nA week ago, Codeup launch...</td>\n",
       "      <td>by dimitri antoniou\\na week ago codeup launche...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10 Tips to Crush It at the SA Tech Job Fair</td>\n",
       "      <td>August 14, 2018</td>\n",
       "      <td>None</td>\n",
       "      <td>SA Tech Job Fair\\nThe third bi-annual San Anto...</td>\n",
       "      <td>sa tech job fair\\nthe third biannual san anton...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Competitor Bootcamps Are Closing. Is the Model...</td>\n",
       "      <td>August 14, 2018</td>\n",
       "      <td>None</td>\n",
       "      <td>Competitor Bootcamps Are Closing. Is the Model...</td>\n",
       "      <td>competitor bootcamps are closing is the model ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title      published_date  \\\n",
       "0  Codeup’s Data Science Career Accelerator is Here!  September 30, 2018   \n",
       "1                                 Data Science Myths    October 31, 2018   \n",
       "2  Data Science VS Data Analytics: What’s The Dif...    October 17, 2018   \n",
       "3        10 Tips to Crush It at the SA Tech Job Fair     August 14, 2018   \n",
       "4  Competitor Bootcamps Are Closing. Is the Model...     August 14, 2018   \n",
       "\n",
       "                                          blog_image  \\\n",
       "0  https://codeup.com/wp-content/uploads/2018/10/...   \n",
       "1  https://codeup.com/wp-content/uploads/2018/10/...   \n",
       "2  https://codeup.com/wp-content/uploads/2018/10/...   \n",
       "3                                               None   \n",
       "4                                               None   \n",
       "\n",
       "                                             content  \\\n",
       "0  The rumors are true! The time has arrived. Cod...   \n",
       "1  By Dimitri Antoniou and Maggie Giust\\nData Sci...   \n",
       "2  By Dimitri Antoniou\\nA week ago, Codeup launch...   \n",
       "3  SA Tech Job Fair\\nThe third bi-annual San Anto...   \n",
       "4  Competitor Bootcamps Are Closing. Is the Model...   \n",
       "\n",
       "                                               clean  \n",
       "0  the rumors are true the time has arrived codeu...  \n",
       "1  by dimitri antoniou and maggie giust\\ndata sci...  \n",
       "2  by dimitri antoniou\\na week ago codeup launche...  \n",
       "3  sa tech job fair\\nthe third biannual san anton...  \n",
       "4  competitor bootcamps are closing is the model ...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codeup_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "codeup_df['stemmed'] = codeup_df.content.apply(tokenize).apply(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/lorisegovia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/lorisegovia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/lorisegovia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/lorisegovia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/lorisegovia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "codeup_df['lemmatized'] = codeup_df.content.apply(tokenize).apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>published_date</th>\n",
       "      <th>blog_image</th>\n",
       "      <th>content</th>\n",
       "      <th>clean</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Codeup’s Data Science Career Accelerator is Here!</td>\n",
       "      <td>September 30, 2018</td>\n",
       "      <td>https://codeup.com/wp-content/uploads/2018/10/...</td>\n",
       "      <td>The rumors are true! The time has arrived. Cod...</td>\n",
       "      <td>the rumors are true the time has arrived codeu...</td>\n",
       "      <td>[the, rumor, are, true, the, time, ha, arriv, ...</td>\n",
       "      <td>[the, rumor, are, true, the, time, ha, arrived...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Science Myths</td>\n",
       "      <td>October 31, 2018</td>\n",
       "      <td>https://codeup.com/wp-content/uploads/2018/10/...</td>\n",
       "      <td>By Dimitri Antoniou and Maggie Giust\\nData Sci...</td>\n",
       "      <td>by dimitri antoniou and maggie giust\\ndata sci...</td>\n",
       "      <td>[by, dimitri, antoni, and, maggi, giust, data,...</td>\n",
       "      <td>[by, dimitri, antoniou, and, maggie, giust, da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Science VS Data Analytics: What’s The Dif...</td>\n",
       "      <td>October 17, 2018</td>\n",
       "      <td>https://codeup.com/wp-content/uploads/2018/10/...</td>\n",
       "      <td>By Dimitri Antoniou\\nA week ago, Codeup launch...</td>\n",
       "      <td>by dimitri antoniou\\na week ago codeup launche...</td>\n",
       "      <td>[by, dimitri, antoni, a, week, ago, codeup, la...</td>\n",
       "      <td>[by, dimitri, antoniou, a, week, ago, codeup, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10 Tips to Crush It at the SA Tech Job Fair</td>\n",
       "      <td>August 14, 2018</td>\n",
       "      <td>None</td>\n",
       "      <td>SA Tech Job Fair\\nThe third bi-annual San Anto...</td>\n",
       "      <td>sa tech job fair\\nthe third biannual san anton...</td>\n",
       "      <td>[sa, tech, job, fair, the, third, biannual, sa...</td>\n",
       "      <td>[sa, tech, job, fair, the, third, biannual, sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Competitor Bootcamps Are Closing. Is the Model...</td>\n",
       "      <td>August 14, 2018</td>\n",
       "      <td>None</td>\n",
       "      <td>Competitor Bootcamps Are Closing. Is the Model...</td>\n",
       "      <td>competitor bootcamps are closing is the model ...</td>\n",
       "      <td>[competitor, bootcamp, are, close, is, the, mo...</td>\n",
       "      <td>[competitor, bootcamps, are, closing, is, the,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title      published_date  \\\n",
       "0  Codeup’s Data Science Career Accelerator is Here!  September 30, 2018   \n",
       "1                                 Data Science Myths    October 31, 2018   \n",
       "2  Data Science VS Data Analytics: What’s The Dif...    October 17, 2018   \n",
       "3        10 Tips to Crush It at the SA Tech Job Fair     August 14, 2018   \n",
       "4  Competitor Bootcamps Are Closing. Is the Model...     August 14, 2018   \n",
       "\n",
       "                                          blog_image  \\\n",
       "0  https://codeup.com/wp-content/uploads/2018/10/...   \n",
       "1  https://codeup.com/wp-content/uploads/2018/10/...   \n",
       "2  https://codeup.com/wp-content/uploads/2018/10/...   \n",
       "3                                               None   \n",
       "4                                               None   \n",
       "\n",
       "                                             content  \\\n",
       "0  The rumors are true! The time has arrived. Cod...   \n",
       "1  By Dimitri Antoniou and Maggie Giust\\nData Sci...   \n",
       "2  By Dimitri Antoniou\\nA week ago, Codeup launch...   \n",
       "3  SA Tech Job Fair\\nThe third bi-annual San Anto...   \n",
       "4  Competitor Bootcamps Are Closing. Is the Model...   \n",
       "\n",
       "                                               clean  \\\n",
       "0  the rumors are true the time has arrived codeu...   \n",
       "1  by dimitri antoniou and maggie giust\\ndata sci...   \n",
       "2  by dimitri antoniou\\na week ago codeup launche...   \n",
       "3  sa tech job fair\\nthe third biannual san anton...   \n",
       "4  competitor bootcamps are closing is the model ...   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  [the, rumor, are, true, the, time, ha, arriv, ...   \n",
       "1  [by, dimitri, antoni, and, maggi, giust, data,...   \n",
       "2  [by, dimitri, antoni, a, week, ago, codeup, la...   \n",
       "3  [sa, tech, job, fair, the, third, biannual, sa...   \n",
       "4  [competitor, bootcamp, are, close, is, the, mo...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  [the, rumor, are, true, the, time, ha, arrived...  \n",
       "1  [by, dimitri, antoniou, and, maggie, giust, da...  \n",
       "2  [by, dimitri, antoniou, a, week, ago, codeup, ...  \n",
       "3  [sa, tech, job, fair, the, third, biannual, sa...  \n",
       "4  [competitor, bootcamps, are, closing, is, the,...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codeup_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ravi's function\n",
    "def prep_article_data(df, column, extra_words=[], exclude_words=[]):\n",
    "    '''\n",
    "    This function take in a df and the string name for a text column with \n",
    "    option to pass lists for extra_words and exclude_words and\n",
    "    returns a df with the text article title, original text, stemmed text,\n",
    "    lemmatized text, cleaned, tokenized, & lemmatized text with stopwords removed.\n",
    "    '''\n",
    "    df['clean'] = df[column].apply(basic_clean)\\\n",
    "                            .apply(tokenize)\\\n",
    "                            .apply(remove_stopwords, \n",
    "                                   extra_words=extra_words, \n",
    "                                   exclude_words=exclude_words)\n",
    "    \n",
    "    df['stemmed'] = df[column].apply(basic_clean)\\\n",
    "                            .apply(tokenize)\\\n",
    "                            .apply(stem)\\\n",
    "                            .apply(remove_stopwords, \n",
    "                                   extra_words=extra_words, \n",
    "                                   exclude_words=exclude_words)\n",
    "    \n",
    "    df['lemmatized'] = df[column].apply(basic_clean)\\\n",
    "                            .apply(tokenize)\\\n",
    "                            .apply(lemmatize)\\\n",
    "                            .apply(remove_stopwords, \n",
    "                                   extra_words=extra_words, \n",
    "                                   exclude_words=exclude_words)\n",
    "    \n",
    "    return df[['title', column,'clean', 'stemmed', 'lemmatized']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Answer the following:\n",
    "- If your corpus is 493KB, would you prefer to use stemmed or lemmatized text?\n",
    "- If your corpus is 25MB, would you prefer to use stemmed or lemmatized text?\n",
    "- If your corpus is 200TB of text and you're charged by the megabyte for your hosted computational resources, would you prefer to use stemmed or lemmatized text?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You would lemmatize because the dataset is small.\n",
    "- You would used the stemmed bc it is much bigger. \n",
    "- Stemmed to save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matthews all in one\n",
    "#def clean_stem_stop(string):\n",
    "   # return remove_stopwords(stem(tokenize(basic_clean(string))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
